\documentclass[11pt,twocolumn]{article}
\bibliographystyle{siam}

\usepackage[margin=0.75in]{geometry}
\usepackage{graphicx}

\title{\textbf{Team Zeta Project Report}\\
Linear Modeling and Time Series Analysis of Visual Pattern Recognition}\\
\author{
  Chen, Tzu-Chieh\\
  \texttt{tcchenbtx}
  \and
  Ho, Edith\\
  \texttt{edithhcw}
  \and
  Marediya, Zubair\\
  \texttt{zubair-marediya}
  \and
  Tran, Mike\\
  \texttt{miketranx4}
  \and
  Zhang, Dongping\\
  \texttt{dpzhang}
}

\begin{document}
\maketitle

\abstract{}
If we present different types of 
visual stimulus to a subject, would each stimulus evoke the 
same category-specific pattern of response in the ventral object vision 
pathway? Furthermore, can we determine and predict individual
categories from the patterns of response evoked? 

\section{Introduction}

In Haxby et al's study, \emph{Distributed and Overlapping Representations of Faces
and Objects in Ventral Temporal Cortex}\cite{objectrec}, researchers collected data 
from six subjects, including five females and one male. 
Different categories of pictures were presented to subjects as visual stimuli.
The categories are faces, cats, chairs, shoes, bottles, tools, houses, 
and a control category of phase-scrambled images.
The study aimed to answer the following questions: if we present 
different categories of visual stimuli to a subject, will category-specific response be 
evoked in the ventral cortex? \\

Each subject was placed into a functional magnetic resonance imaging 
(fMRI) facility for 12 times. One complete experiment run lasted 300s. It began 
with 12s of rest, followed by 8 stimulus blocks of 24s duration, one for 
each category of visual stimuli. There were 12s intervals of rest between 
every two blocks, and the whole procedure ended with another 12s of rest. 
Each picture stimulus were presented for 0.5s followed by an inter-stimulus 
interval of 1.5ms. 12 stimuli were presented during each stimulus 
block, and a total of 96 stimuli in a complete experiment run.\\

In the study, the data collected for each subject were split into two 
runs: odd runs and even runs. Correlation was used as the indication of 
response similarity. Results from analyzing within-category and 
between-category correlations suggest that a lot of response patterns 
are overlapping. In the overlaps, there are parts of the cortex that respond 
more to a certain stimuli than others. The study defines these responses as 
maximal response. In order to gain more insights from the non-overlapping 
parts of the responses, the study tests whether the patterns of non-maximal 
responses carry category-related information. Voxels that responded maximally 
to were excluded from this calculation of correlations. The results show that the 
removal of maximally responsive voxels from correlation calculations 
barely diminishes the accuracy of identification. The study concludes 
that both the pattern of large and small responses and the location 
of large responses carry category-related information; and small responses 
are an integral part of the representation.\\

\section{Data}

The study's curated dataset can be found and downloaded on the OpenfMRI 
database with ds105's accession number. The ds105 
sub-directory contains files detailing this study, including general information 
(README file), related research articles (references.txt), detail information 
and update for this released dataset (release\textunderscore history.txt), 
the MR repetition time (scan\textunderscore key.txt), the name 
(study\textunderscore key.txt), and the major task for this study 
(object viewing) (task\textunderscore key.txt). In addition, the models folder 
contains files with the key conditions (list of object categories) 
(condition\textunderscore key.txt) and the comparison setting in this study 
(tast\textunderscore contrasts.txt). \\

Subjects have individual directories storing their results. There are four 
sub-directories in each of the respective directories. The anatomy 
sub-directory contains high-resolution scans of the subject's head 
(highres001.nii.gz), mask for obtaining the ``brain only'' scans 
(highres001\textunderscore brain\textunderscore mask.nii.gz), and the 
``brain only'' anatomy result (highres001\textunderscore brain.nii.gz). 
The ``behav" sub-directory is empty since subject's behavior is irrelevant 
to this study. The ``model" sub-directory provides information such as the 
onset time (in seconds), and the duration and weighting for each conditions 
(object category) for the 12 task runs in this study. The ``BOLD" sub-directory 
contains fMRI results for all 12 task runs for each subject respectively. 
In each task run directory, we can find the fMRI result (bold.nii.gz) and 
a QA sub-directory with that run's time series analysis report, fMRI results
pre-processing and confound files, and visualization of the brain (nii files).\\ 

\section{Methods}

For each subject, there are eight condition files corresponding 
to each of the eight objects for each of the twelve runs. Each
condition file consists of time points of when the object was presented 
during the run. We started our analysis with subject 1, where we first wrote 
helper functions to identify and remove outliers.  Then we ran 
the event2neural function on subject 1's condition files, which returned an 
array of 121 zeros or ones. These values indicate
time intervals at which the subject was looking at the object.
We then used the numpy convolve function to convolve the BOLD signal 
onto the specified time intervals needed. After repeating this process 
for all objects, we created regressors and built our design matrix. 
Lastly, we fitted a linear model to the predicted BOLD signals 
from convolution, and gathered a mean RSS value. We then applied the contrast
function to investigate whether each object shows a response pattern in the brain.
Unfortunately, initial images produced did not show sufficient contrast, 
making it difficult to observe patterns. Thus, we used smoothing techniques to 
produce better images. For smoothing, we applied Gaussian filters over an
array of voxel intensity values to generate images that are more appealing
to the human eye and easier to identify different parts of the brain.
The downside of this technique is that the new patterns identified can
potentially be false since the data is transformed. We then repeated our 
data analysis with the rest of the subjects.
 \\

We used various statistical methods and tests in our analysis. First,
we built a general linear model to provide estimates for the magnitude of
the response for respective stimulus. Then we applied this model to obtain
beta values for different objects. We then ran correlation analysis on the
beta values, which is further explained below. Moreover, the BOLD signal 
data are time series, and we performed appropriate analysis such as 
an object correlation table for each subject and an autoregressive integrated
moving average model to predict and fit the BOLD signal responses. 
Finally, in order to validate our models and data analysis, we looked 
at mean squared errors to judge how well our model performs. \\

For correlation analysis, we divided all the runs into two groups (odd and even)
, and we aggregated all the results from each object within the group. Then, 
we computed the correlations between the average beta values of each object 
of each group and all objects in the opposite group. Take all the even runs 
where the subject was shown an image of a face as an example. We take 
the compiled array of beta values, and found the correlation between this 
array and that generated from all the odd runs where the subject was shown 
the images of a face, bottle, cat, chair, house, scissors, scrambledpix, and 
shoe respectively. This gives us 8 correlation values. By repeating the same 
procedures with all the other objects, we created a 8x8 correlation matrix. \\

\section{Results}

Since we started our analysis with subject 1, run001, we will first display those
results. We performed initial analysis to attempt to identify specific brain region 
for recognizing specific object, such as face or house. 

Here are the outliers we identified and removed. \\
\begin{figure}[h!]
\centering
\includegraphics[width=90mm]{Volume_RMS_Difference_Outliers_sub001_run001.png}
\caption{Remove Outliers}
\end{figure}

Then, we generated task time course with the event2neural function.
\begin{figure}[h!]
\centering
\includegraphics[width=90mm]{Task_time_course_sub001_run001.png}
\caption{Task Time Course}
\end{figure}

\pagebreak 

Afterwards, we performed convolution to generate predicted BOLD signals for this 
dataset.
\begin{figure}[h!]
\centering
\includegraphics[width=100mm]{sub001_run001_bold_prediction.png}
\caption{Stimulation Bold}
\end{figure}

These convolved results for each objects were used as parameters in 
our design matrix for linear regression. To avoid the drifting problem, we also 
included two drift parameters in the design matrix. The final design matrix was 
arranged as followed: bottle, cat, chair, face, house, scissors, scrambledpix, 
shoe, drift1, drift2, average(all ones). 
\begin{figure}[h!]
\centering
\includegraphics[width=90mm]{design_matrix_sub001_run001.png}
\caption{Design Matrix}
\end{figure}

As mentioned above, we ran correlation analysis on the beta values.
None of the correlation values are perfectly 1. The diagonal values 
are correlations between each object with their respective odd or 
even runs (within-category correlations). All within-category correlation 
values are larger than 0.6. For bottle, cat, face, house, scrambledpix, 
and shoe, the respective within-category correlations are higher than 
all of that object?s between-category correlations with other objects. 
For chair and scissors, the within-category correlations are not higher 
than all of their between-category correlations.
\begin{figure}[h!]
\centering
\includegraphics[width=90mm]{3d_correlation_table_sub001.png}
\caption{3D Correlation Table for Subject 1}
\end{figure}

\begin{figure}[h!]                                                              
\centering                                                                      
\includegraphics[width=90mm]{3d_non_max_correlation_table_sub001.png}                   
\caption{3D Correlation Excluding Maximally Responding Voxels for Subject 1}                                    
\end{figure}

In the correlation analysis, some questions we wanted to answer were
would the time series correlation tables be similar to ones produced from
the linear regression process if we looked at the raw nii data, how would
the correlations differ between the same and different objects, and was the
experiment designed in such a way so that correlations are the approximately
the same with different runs and subjects. The first step in the time series
analysis was retrieving a particular subset of the brain from the raw data.
This was done by getting the entire 4-D data, applying a mask on the 4-D data
to remove any noises, and using the same subsetted brain as the linear
regression process. Then, we averaged the subsetted brain into an array that
represents the BOLD signal responses from the beginning to the end of each run.
The array has a length of 121, which is the same as the length of each run.
Next, based on the condition files of each subject and run, we segmented the
array and created a time series array for each of the eight objects. The
condition files specified when an object was shown and for how long; thus,
giving us a particular range to subset precisely. Finally, we grouped the
object time series arrays into either even or odd runs. This allows us to
compare averaged even and odd object runs to one another and generate
Pearson Correlation coefficients. At the end of this analysis, six correlation
tables were generated.

\begin{figure}[h!]                                                              
\centering                                                                      
\includegraphics[width=90mm]{sub001_run001_corrFunc.png}                   
\caption{Autocorrelation and Partial Autocorrelation for Subject 1 Run 1}                                    
\end{figure}


On the time-side analysis, we were able to fit autoregressive integrated
moving average (ARIMA) models to the BOLD signal responses. This allowed us to
see how good of a fit a time series model is compared to the raw data and 
predict BOLD signal responses without having to know an object a subject is 
looking at. To see if we could initally fit a time-side model, we looked at
subject one run one's data. An autocorrelation and partial autocorrelation 
plot of the run's raw BOLD signals was generated. 

There is a significant lag cut-off after lag two in the partial autocorrelation plot, 
which means the autoregressive order should be at least two. The cut-off in 
the autocorrelation plot was at lag four, indiciating the moving average 
order should be four. To ensure these are the right orders for the ARIMA model, 
an auto.Arima() function was used to automate the parameter search process. 
In subject one run one, the best ARIMA model was ARIMA(2, 0, 0). 
Upon fitting the model, the residual's autocorrelation and partial 
autocorrelation plot were looked at to ensure a goodness of fit. 

\begin{figure}[h!]                                                              
\centering                                                                      
\includegraphics[width=90mm]{sub001_run001_residcorrFunc.png}                   
\caption{Autocorrelation and Partial Autocorrelation for Subject 1 Run 1 Residuals}                                    
\end{figure}

In both plots, the significant lag cuts off after lag one;
thus, showing that the residuals have no serial correlation. In addition, the
Ljung-Box test was used to validate this assumption. It has a null hypothesis
that the residuals are independently distributed and uncorrelated, with an
altnerative hypothesis that they are correlated. The p-value came out to be
0.6071, so we accept the null. One trend that came up was that there were
no periods of volatility in the residuals plot; thus, we could not look into
an ARIMA-GARCH hybrid model.

\begin{figure}[h!]                                                              
\centering                                                                      
\includegraphics[width=90mm]{sub001_run001_residFit.png}                   
\caption{Residuals of ARIMA Model}                                    
\end{figure}

\begin{figure}[h!]                                                              
\centering                                                                      
\includegraphics[width=90mm]{AFTS.png}                   
\caption{Actual vs Fitted Values of Subject 1 Run 1}                                    
\end{figure}


\section{Discussion}

Most of the within-category correlations we found are fairly similar than the paper?s. 
Some of the values from our analysis are higher than the paper?s, which could be 
due to the fact that we only analyzed correlations between runs for one subject, 
while the original research paper studied all subjects. It would make sense that 
correlations are higher when only analyzing brain activity from one brain. \\

Correlations between different objects (between-category correlations) seem 
fairly random, which also seem to be in line with the original paper?s finding. 
We observe that our between-category correlation values also tend to be higher 
than those from the paper. \\

To test the reproducibility of the study, we also redid our correlation analysis 
after excluding the maximally-responsive voxels. With the exception of some 
within-category correlations, most correlation values across all categories dropped 
after we removed the maximally-responsive voxels. This might be an indication 
that most of the highly correlated areas tend to have higher responses. \\

Looking at the correlation tables, the first thing that we noticed was that
the time series correlation tables were much more different from the linear
regression correlation tables. Even though the same subset of the brain was 
used, the time series tables exclusively had negative correlations, higher 
correlations with dissimilar objects against same objects, and even negative
correlations with the same objects. \\

\begin{figure}[h!]                                                              
\centering                                                                      
\includegraphics[width=90mm]{subtracted_correlation_table_sub001.png}                   
\caption{3D Correlation Time Series Table for Subject 1}                                    
\end{figure}

\begin{figure}[h!]                                                              
\centering                                                                      
\includegraphics[width=90mm]{subtracted_correlation_table_sub003.png}                   
\caption{3D Correlation Time Series Table for Subject 3}                                    
\end{figure}


These differences could have come from how
the two processes were executed. For example, the linear regression process
involved convolving the data and transforming the data with beta values, 
whereas the time series analysis only looked at the raw data. Thus, this is
the most likely reason as to why the tables differed. The second thing the time
series correlation tables had was that across the six subjects, no two subjects
had similar correlation results. This confirms one of our hypothesis that 
the brain is a complex structure that will never give consistent results even
with a well designed experiment. What this means is that the BOLD signal
responses are not exactly 100\% comparable and it should be expected to have
contrasting correlation coefficients. One last point to bring up is that
the BOLD signals were not consistent across a subject's runs. In one run, a
cat BOLD signal would come out at an intensity of 10,800 and then in another,
it would come out to 10,500. One question we wanted to answer was given the
objects, can we predict the BOLD signal responses? Given that an object's
BOLD signal was never the same, this showed us that we couldn't answer that
particular question. Thus, we were unable to conclude anything regarding
predicting a BOLD signal given the objects. \\

In terms of time series analysis, now that there is a good model 
for one particular run, we can go about
answering the original questions we had with time-side analysis. When we 
plotted the actual to the fitted values, we see that the ARIMA model was 
able to capture the actual values very well. This allows us to get into
forecasting and predicting future responses. However, one limiation with
the python StatsModel package is that there are no forecasting methods
available. Thus, no further progress could be done unless R was available
as a tool. \\

There are a lot of problems that came up during our analysis. To begin with, 
since most of us have limited neuroscience knowledge, it was very difficult 
just to understand the study and the data itself. The research paper of the 
study uses specific and technical terms that make it hard to comprehend. 
More time was spent at the earlier stages rereading the 
original paper and exploring the data folders than expected, which slowed down 
our progress for analysis. Moreover, the values from correlation analysis 
did not come out as clean as we hoped, which also set our progress back. 
We did not end up have time to apply machine learning techniques for 
predictions as we originally splanned to. \\

\bibliography{project}

\include{appendix1_general}
\include{appendix2_subject1}
\include{appendix3_cross_subject}

\end{document}
